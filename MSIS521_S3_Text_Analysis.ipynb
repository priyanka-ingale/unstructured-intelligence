{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanka-ingale/unstructured-intelligence/blob/main/MSIS521_S3_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CebOULrb5m-0"
      },
      "source": [
        "# Text Analysis\n",
        "This example shows a few text analyis methods: similarity, clustering, topic modeling and sentiment analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "We are providing the data as a list of documents."
      ],
      "metadata": {
        "id": "xKMdWa5YDP-Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xaacw2D5m-1"
      },
      "outputs": [],
      "source": [
        "d0 = \"He is a good guy, he is not bad\"\n",
        "d1 = \"feet wolves cooked boys girls ,!<@!\"\n",
        "d2 = \"He is not a good guy, he is bad\"\n",
        "d3 = \"I drink water in parties\"\n",
        "d4 = \"I grab a drink in parties\"\n",
        "\n",
        "c3 = [d3, d4]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity Measures"
      ],
      "metadata": {
        "id": "8zTmdA3PDkn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import * # Cosine\n",
        "from sklearn.metrics import * # Jaccard"
      ],
      "metadata": {
        "id": "0nDunHRtDm78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CountVectorizer"
      ],
      "metadata": {
        "id": "tAl-PyR9utPF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUAlvrXi5m-2"
      },
      "source": [
        "\n",
        "`fit()` finds BOW (Bag of Words) and generate vocabulary (indexed alphabetically)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJLPyBmi5m-2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3yXJIAv5m-3"
      },
      "source": [
        "`transform()` converts each word to a vector spanned by the vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4R5rtdJT5m-3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT1w6gvQ5m-3"
      },
      "source": [
        "**Similarity** - cosine_similarity needs input as lists.\n",
        "We can calculate Jaccard similarity because the DTM is binary. (We could have forced binary DTM by using vectorizer5 = CountVectorizer(binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm0r2jtv5m-3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIQZIQBp5m-3"
      },
      "source": [
        "## Clustering Analysis\n",
        "\n",
        "We use K-Means and Agglomerative Clustering for clustering analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer # we will use TF-IDF vectorizer\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "EegQlu7M58tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlZz0dZY5m-4"
      },
      "outputs": [],
      "source": [
        "d5 = \"Seattle weather is bad in winter\"\n",
        "d6 = \"Seattle Seahawks is a great football team\"\n",
        "d7 = \"I love Seahawks\"\n",
        "d8 = \"I learned a lot of Data analytics tools\"\n",
        "d9 = \"I am a data scientist\"\n",
        "c4 = [d1,d2,d3,d4,d5,d6,d7,d8,d9,d10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cvVb4D8U6A8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paso6lE25m-4"
      },
      "source": [
        "### K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`fit()` to train the model which gives cluster_centers_"
      ],
      "metadata": {
        "id": "il4-xhuQexX3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mYx4vEy5m-4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2nemt2A5m-4"
      },
      "source": [
        "`transform()` assigns cluster membership as given by labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNw6oHYp5m-4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjZiSGn75m-4"
      },
      "source": [
        "#### Characteristics of each cluster\n",
        "\n",
        "We want to find **5 most significant words for each cluster**.\n",
        "\n",
        "1. for each cluster, we get the indices of the sorted array in descending order. `argsort()` sorts the array in ascending order and return the indices of sorted array. The index slicing  `[:,::-1]`reads the sorted array from the end and effectively re-arranges it in descending order.\n",
        "2. `get_feature_names_out()` returns the words corresponding to indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2Xh6atb5m-4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0jlLRYe5m-5"
      },
      "source": [
        "### Agglomerative Clustering\n",
        "\n",
        "1. **n_clusters** - number of clusters pre-selected\n",
        "2. **affinity** - distance measure between documents, others are: manhattan, cosince\n",
        "3. **linkage** - distance measure between clusters, others are: single, complete, average  \n",
        "\n",
        "`fit_predict()` fits the hierarchical clustering from features or distance matrix, and returns cluster labels."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SHTF8Q689MkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJtA2ByW5m-5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Packages to plot dendrogram\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import plotly.figure_factory as ff"
      ],
      "metadata": {
        "id": "anqMDAeGratP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z = linkage(X.toarray(), method=\"complete\")\n",
        "fig_complete = ff.create_dendrogram(Z, orientation='bottom')\n",
        "fig_complete.update_layout(width=1000, height=600, title='Hierarchical Clustering Dendrogram using Complete Linkage')\n",
        "fig_complete.show()"
      ],
      "metadata": {
        "id": "sYhfdMq-rbX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwJ6k1FV5m-5"
      },
      "source": [
        "## LDA Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data and document vectors"
      ],
      "metadata": {
        "id": "PqCyvWgr9uz_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMBHCQI65m-5"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "irjw10tFC76P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C_qdCA15m-5"
      },
      "source": [
        "### Latent Dirichlet Allocation\n",
        "\n",
        "* **n_components** - number of topics\n",
        "* The model **lda**'s attribute **components_** stores topic word distribution. The array **components_[i, j]** can be viewed as pseudocount that represents the number of times **word j** was assigned to **topic i**.\n",
        "\n",
        "To display the representative words under each topic, for each topic:\n",
        "1. sort the indices of words in the descending order of pseudocount\n",
        "2. take the top 4\n",
        "2. retrieve the corresponding words and join them with space \" \" inbetween them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBTMieQ75m-5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gr0JW345CVf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AJJT8pe5m-5"
      },
      "source": [
        "#### Probability of a document containing a topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZNXiDF25m-5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwgxjksC5m-5"
      },
      "source": [
        "### LDA using gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare the data\n",
        "\n",
        "list of lists of words from list of documents"
      ],
      "metadata": {
        "id": "maMmuPXWcXmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "xj_J9-ObN1H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roL3Tur85m-5"
      },
      "outputs": [],
      "source": [
        "## Preprocessing the documents for gensim\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "processed_c4 = []\n",
        "for doc in c4:\n",
        "  tokens = nltk.word_tokenize(doc.lower())\n",
        "  tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
        "  tokens = [stemmer.stem(token) for token in tokens]\n",
        "  tokens = [token for token in tokens if not token in stopwords.words('english')]\n",
        "  processed_c4.append(tokens)\n",
        "\n",
        "print(processed_c4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBpHZOnm5m-5"
      },
      "source": [
        "#### gensim\n",
        "\n",
        "In terminal, **pip install gensim**\n",
        "\n",
        "1. create the **dictionary** which is a list of words from all documents\n",
        "2. convert document into the bag-of-words format = list of (word_id, word_count) 2-tuples\n",
        "3. train the model with number of topics **num_topics** and mapping from word IDs to words **id2word**\n",
        "4. use **`print_topics(num_topics, num_words)`** to display **num_topics** randomly selected topics with a string of **num_words** words ordered by their significance. Both are optional with the default `num_topics=20, num_words=10`. `num_topics = -1` means to show all topics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import gensim"
      ],
      "metadata": {
        "id": "zyrujVB-OWtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvbmSVLj5m-5"
      },
      "outputs": [],
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_c4)\n",
        "bow_c4 = [dictionary.doc2bow(doc) for doc in processed_c4]\n",
        "print(bow_c4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = gensim.models.LdaModel(bow_c4, num_topics=4, id2word=dictionary\n",
        "                                   ,passes=10\n",
        "                                   ,iterations=200)"
      ],
      "metadata": {
        "id": "kW4XAUkrSqqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model.print_topics(-1,4):\n",
        "  print(f'Topic {idx}: \\n Words: {topic}')"
      ],
      "metadata": {
        "id": "-Plw10KZCwc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8vZQ5eL5m-5"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install vader_lexicon"
      ],
      "metadata": {
        "id": "QmuwCNqodfgU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlR5HSxL5m-5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvIbvcnz5m-6"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GCt408k5m-6"
      },
      "outputs": [],
      "source": [
        "sentences = [\"They are smart, cute, and funny.\",  # positive sentence example\n",
        "    \"They are smart, cute, and funny!\", # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
        "    \"They are very smart, cute, and funny.\",# booster words handled correctly (sentiment intensity adjusted)\n",
        "    \"They are VERY SMART, cute, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
        "    \"They are VERY SMART, cute, and FUNNY!!!\",  # combination of signals - VADER appropriately adjusts intensity\n",
        "    \"They are VERY SMART, really handsome, and INCREDIBLY FUNNY!!!\",  # booster words & punctuation make this close to ceiling for score\n",
        "    \"The book was good.\",  # positive sentence\n",
        "    \"The book was kind of good.\",  # qualified positive sentence is handled correctly (intensity adjusted)\n",
        "    \"The plot was good, but the characters are uncompelling and the dialog is not great.\",  # mixed negation sentence\n",
        "    \"A really bad, horrible book.\",  # negative sentence with booster words\n",
        "    \"At least it isn't a horrible book.\",  # negated negative sentence with contraction\n",
        "    \":) and :D\",  # emoticons handled\n",
        "    \"\",  # an empty string is correctly handled\n",
        "    \"Today sux\",  # negative slang handled\n",
        "    \"Today sux!\",  # negative slang with punctuation emphasis handled\n",
        "    \"Today SUX!\",  # negative slang with capitalization emphasis\n",
        "    \"Today kinda sux! But I'll get by, lol\"  # mixed sentiment example with slang and contrastive conjunction \"but\"\n",
        "     ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0SddWZ35m-6"
      },
      "source": [
        "### Sentiment analysis with existing classifier\n",
        "\n",
        "1. use `SentimentIntensityAnalyzer()`\n",
        "2. get polarity scores: compound, neg, neu, pos. The output is a dictionary data type which is joined by a dictionary of sentence using `update()`.\n",
        "3. convert list of dictionary to `DataFrame` and show."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouyQizZp5m-6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH93krk75m-6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}