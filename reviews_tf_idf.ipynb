{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "reviews-tf-idf.ipynb",
      "authorship_tag": "ABX9TyOiLJH5RQgt9PHPaibJnz/l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanka-ingale/unstructured-intelligence/blob/main/reviews_tf_idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF vector representation of reviews"
      ],
      "metadata": {
        "id": "rzkr8srN-7J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "3bNV5pdo7D4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCQ3Da7ryauW"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('IA1_1.csv', header=None, names=['id', 'review'])\n",
        "reviews = df['review'].astype(str).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "wH6DNWZk93O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkjGqSev-wak",
        "outputId": "cb704bca-fbaa-4325-d3d7-ee3d674adafa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize NLP tools\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "Y_ZoNkk0_RvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Steps 1 to 3: Tokenization, Lemmatization, and Cleaning ---\n",
        "processed_reviews = []\n",
        "tokenized_reviews_step1 = [] # Kept for Step 5\n",
        "\n",
        "for review in reviews:\n",
        "    # Step 1: Tokenize\n",
        "    tokens = nltk.word_tokenize(review.lower())\n",
        "    tokenized_reviews_step1.append(tokens)\n",
        "\n",
        "    # Step 2: Lemmatize\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Step 3: Remove stop-words and non-alphabetic tokens (replaces punctuation check)\n",
        "    cleaned = [word for word in lemmatized if word not in stop_words and word.isalpha()]\n",
        "\n",
        "    processed_reviews.append(\" \".join(cleaned))"
      ],
      "metadata": {
        "id": "lxuywwjh_Y7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: TF-IDF Vectors (Min DF=3, include 2-grams) ---\n",
        "vectorizer4 = TfidfVectorizer(min_df=3, ngram_range=(1, 2))\n",
        "tfidf_matrix4 = vectorizer4.fit_transform(processed_reviews)"
      ],
      "metadata": {
        "id": "rx6Y9kmpAQZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame and save to CSV\n",
        "df_step4 = pd.DataFrame(tfidf_matrix4.toarray(), columns=vectorizer4.get_feature_names_out())\n",
        "df_step4.to_csv('tfidf_vectors_step4.csv', index=False)"
      ],
      "metadata": {
        "id": "kojGNodRBCyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAEwgttTEmcm",
        "outputId": "b934b69b-8875-4fff-b7ea-30a984e92318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5: POS-tag TF-IDF Vectors (Min DF=4, no normalization/stop-word removal) ---\n",
        "# Use tokens directly from Step 1\n",
        "pos_tagged_reviews = []\n",
        "for tokens in tokenized_reviews_step1:\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "    # Format: word_TAG\n",
        "    tagged_string = \" \".join([f\"{word}_{tag}\" for word, tag in tags])\n",
        "    pos_tagged_reviews.append(tagged_string)"
      ],
      "metadata": {
        "id": "EFk7WQHIBc6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization without normalization or stop-word removal\n",
        "vectorizer5 = TfidfVectorizer(min_df=4, norm=None, stop_words=None)\n",
        "tfidf_matrix5 = vectorizer5.fit_transform(pos_tagged_reviews)"
      ],
      "metadata": {
        "id": "GZz7cniLEzQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame and save to CSV\n",
        "df_step5 = pd.DataFrame(tfidf_matrix5.toarray(), columns=vectorizer5.get_feature_names_out())\n",
        "df_step5.to_csv('pos_tfidf_vectors_step5.csv', index=False)"
      ],
      "metadata": {
        "id": "d5l-3cOSE770"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Reporting Dimensions ---\n",
        "print(f\"Step 4 Vector Dimension: {tfidf_matrix4.shape[1]}\")\n",
        "print(f\"Step 5 Vector Dimension: {tfidf_matrix5.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snqLW6QuFAXM",
        "outputId": "f68e1433-608f-438b-fa9a-49ec6a487347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4 Vector Dimension: 1322\n",
            "Step 5 Vector Dimension: 936\n"
          ]
        }
      ]
    }
  ]
}